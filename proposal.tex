\documentclass{article}

% TODO
% - clean-up the background sections

% \usepackage[switch]{lineno}
% \renewcommand\thelinenumber{\textcolor{red}{\arabic{linenumber}}}
\usepackage{xcolor}
\usepackage[utf8]{inputenc} % Use UTF-8 encoding
\usepackage[T1]{fontenc} % Use the T1 font encoding
\usepackage{geometry} % Adjust page margins
\usepackage{amsmath, amssymb} % For math symbols and equations
\usepackage{graphicx} % For including images
\usepackage{titlesec} % For customizing section titles
% \usepackage[style=authoryear]{biblatex}  % For managing references
\usepackage[round, square]{natbib}
\bibliographystyle{plainnat}
\usepackage{hyperref}
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{tabularx}   %% For tables with growable column
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{cleveref}   %% For smart ref
\usepackage{graphicx}   %% For inserting images
\graphicspath{ {./images/} }  %% configure the graphicx package
\usepackage{multirow}
\usepackage{multicol}
\usepackage{ragged2e}  %% For \raggedright
\usepackage{listings}  %% For typesetting code
\usepackage{enumitem}  %% For noitemsep and topsep
\lstdefinelanguage{smol} {
  alsoletter={ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-!?*},
  morekeywords={defvar,deffun,lambda,set!,let,let*,letrec,begin,if,cond,else,and,or},
  morecomment=[l]{;},
  morestring=[b]",
}
\lstset{
  basicstyle=\small\ttfamily,
  breaklines=true,
  language=smol
}

\newcommand{\miscon}[1]{\textbf{#1}}
\newcommand{\surmisedMiscon}[1]{\miscon{#1}$^\ddag$}
\newcommand{\newterm}[1]{\textbf{#1}}

%% abbrev. for p-value
\newcommand{\pValue}[0]{\textit{p}\nobreakdash-value}

%% abbrev. for RQs
\newcommand{\rqI}{What program-behavior misconceptions apply to SMoL and present (even) in students with prior programming backgrounds?}
\newcommand{\rqII}{What would be a tutoring system that aims to correct the misconceptions, draws on cognitive, educational, or psychological concepts, and seems to be effective?}

%% abbrev. for JavaScript etc.
\newcommand{\js}{JavaScript}
\newcommand{\py}{Python}

%% abbrev. for tutorials
\newcommand{\TutDef}{The \textbf{def} tutorial}
\newcommand{\TutSet}{The \textbf{set} tutorial}
\newcommand{\TutVec}{The \textbf{vec} tutorial}
\newcommand{\TutLam}{The \textbf{lam} tutorial}
\newcommand{\TutLet}{The \textbf{let} tutorial}
\newcommand{\tutDef}{the \textbf{def} tutorial}
\newcommand{\tutSet}{the \textbf{set} tutorial}
\newcommand{\tutVec}{the \textbf{vec} tutorial}
\newcommand{\tutLam}{the \textbf{lam} tutorial}
\newcommand{\tutLet}{the \textbf{let} tutorial}

\title{Dissertation Proposal}
\author{Kuang-Chen Lu}
\date{\today}

\begin{document}
\maketitle
% \linenumbers

\begin{abstract}
  Misconceptions about core linguistic concepts like mutable
  variables, mutable compound data, and their interaction with scope
  and higher-order functions seem to be widespread. But how do we
  detect them, given that experts have blind spots and may not realize
  the myriad ways in which students can misunderstand programs?
  Furthermore, once identified, what can I do to correct them?

  I propose a plan for finding misconceptions possibly not anticipated
  by experts, and the design of an automated, self-guided tutoring
  system. The Tutor builds on strategies in the cognitive and
  educational literature and is explicitly designed around identifying
  and correcting misconceptions. Preliminary results suggest (a) the
  misconceptions I found are widespread, and (b) the Tutor appears to
  improve understanding.
\end{abstract}

\maketitle

\clearpage

\tableofcontents

\clearpage

\section{Introduction}
\label{s:intro}

A large number of widely used modern programming languages share a
common semantic basis:
\begin{itemize}

  \item lexical scope
  \item nested scope
  \item eager evaluation
  \item sequential evaluation (per ``thread'')
  \item mutable first-\emph{order} variables
  \item mutable first-\emph{class} structures (objects, vectors, etc.)
  \item higher-order functions that close over bindings
  \item automated memory management (e.g., garbage collection)

\end{itemize}
This semantic core can be seen in languages from ``object-oriented''
languages like C\# and Java, to ``scripting'' languages like
JavaScript, Python, and Ruby, to ``functional'' languages like the ML
and Lisp families. Of course, there are sometimes restrictions (e.g.,
Java has restrictions on closures) and extensions (such as the
documented semantic oddities of JavaScript and Python
\citep{bernhardtWat2012, guhaEssenceJavascript2010,
politzPythonFullMonty2013, politzTestedSemanticsGetters2012}). Still,
this semantic core bridges many syntaxes, and understanding it helps
when transferring knowledge from old languages to new ones. In
recognition of this deep commonality, in this proposal I choose to call
this the \emph{Standard Model of Languages} (SMoL).

Unfortunately, this combination of features appears to also be non-trivial for
programmers to understand. Consider the following scenario: to create a
calculator, I have to construct a callback that can be attached to
each button. The following Python program seems to achieve the
construction of these callbacks and the act of pushing the buttons in
order:
\begin{lstlisting}[language=Python]
button_list = []

for i in range(10):
    button = lambda: print(i)
    button_list.append(button)

for button in button_list:
    button()
\end{lstlisting}
That is, a user would expect to see \lstinline|0| through \lstinline|9|. In
fact, however, it prints \lstinline|9| ten times.

As a background section (\Cref{s:background-misconceptions}) describes,
multiple researchers, in different
countries and different kinds of post-secondary educational contexts,
have studied how students fare with scope and state. They consistently
find that even advanced students have difficulty with such programs
and even programs simpler than this.

In fact, these problems are not at all limited to students. This
specific looping problem even trips up industrial programmers. The
C\# language \emph{changed} to produce \lstinline|0| through \lstinline|9| \citep{lippertClosingLoopVariable2009}. It is now also the focus of a language design
change in Go \citep{chaseFixingLoopsGo2023} for a new kind of looping
construct \citep{coxSpecLessErrorprone2023}.
It continues to trip up programmers
(e.g., \cite{sharveyCreatingFunctionsLambdas2022})
despite being
documented as a ``gotcha'' in Python \citep{reitzHitchhikerGuidePython2016}.
% SK: But not everyone has read the document. I don't think the document is
% a Python official document. Also, this sentence sounds like stating that
% the cited Python programmers are stupid.

Most of the time, however, these misunderstandings do not lead to
language design changes. Nor are changes necessarily desirable: the
SMoL feature set has presumably evolved because it is convenient
for writing non-trivial programs (e.g., mutable structures) without
being too unwieldy (e.g., no dynamic scope). Furthermore, having a
good mental model of these features
is essential to understand ownership \citep{clarkeOwnershipTypesFlexible1998},
manage parallelism, and more. Thus, we
still need to train students and other programmers on the semantic
features \emph{and their interactions} in SMoL.

This led to my \textbf{research questions}:

\begin{description}
  \item[RQ1] \rqI
  \item[RQ2] \rqII
\end{description}

In the rest of this document, I first define SMoL (\Cref{s:smol}),
then present a literature review of misconceptions
(\Cref{s:background-misconceptions}) and tutoring systems
(\Cref{s:background-tutoring-systems}), and then outline my research
plans for the RQs (\Cref{s:plan-rq1,s:plan-rq2}). The remaining
sections detail my current progress.

\section{The SMoL Language}\label{s:smol}

\begin{figure}[t]
  \centering
  \begin{lstlisting}
    t ::= d
        | e
    d ::= (defvar x e)
        | (deffun (f x ...) body)
    e ::= c
        | x
        | (lambda (x ...) body)
        | (let ([x e] ...) body)
        | (begin e ... e)
        | (set! x e)
        | (if e e e)
        | (cond [e body] ... [else body])
        | (cond [e body] ...)
        | (e e ...)
    body    ::= t ... e
    program ::= t ...
  \end{lstlisting}
  \caption{The syntax of SMoL.}
  \label{f:smol-syntax}
\end{figure}

\begin{table}[t]
  \centering\begin{tabularx}{\linewidth}{r X}
    \textbf{Operators}            & \textbf{Meaning}                                       \\\hline
    \texttt{+ - * /}              & Arithmetic Operators                                   \\
    \texttt{< > <= >=}            & Number comparison                                      \\
    \texttt{mvec}                 & Create a (mutable) vector (a.k.a. array)               \\
    \texttt{vec-ref}              & Look up a vector element                               \\
    \texttt{vec-set!}             & Replace a vector element                               \\
    \texttt{vec-len}              & Get the length of a vector                             \\
    \texttt{mpair}                & Create a 2-element (mutable) vector                    \\
    \texttt{left right}           & Look up the first/second element of a 2-element vector \\
    \texttt{set-left! set-right!} & Replace the first/second element of a 2-element vector \\
    \texttt{eq?}                  & Equality                                               \\
  \end{tabularx}
  \caption{Primitive operators of SMoL.}
  \label{t:smol-primitives}
\end{table}

SMoL is designed to capture common features of many modern languages.
The syntax of SMoL is presented in~\Cref{f:smol-syntax}, where
\lstinline|t| stands for terms, \lstinline|d| stands for definitions,
\lstinline|e| stands for expressions, \lstinline|c| stands for
constants (i.e., number, boolean, and string), and \lstinline|x| and
\lstinline|f| are identifiers (variables). The last kind of expression
(i.e., \lstinline|(e e ...)|) is function application.

SMoL defines a semantic but not the syntax. However, to represent SMoL
to students, I have to choose a syntax. I chose a Lispy syntax, which
is an artifact of the major population where I conducted studies: a
course (\Cref{s:populations}) using Racket
\citep{friedmanEssentialsProgrammingLanguages2001,
krishnamurthiProgrammingLanguagesApplication2007}. However, the Lispy
syntax also proved to be pedagogically valuable. I have found the
parentheses useful when discussing scope in conjunction with
local-binding features like \lstinline|let|. This avoids the various
confusing ``variable lifting'' semantics found in languages like
Python (\cite{politzPythonFullMonty2013}; see also posts like
\cite{froadieWhatScopeVariable2022}), where the actual defined range
of a variable is not apparent from the source code.

Nevertheless, most SMoL programs are easy to translate to other
languages. Furthermore, I intend to make a multi-lingual Tutor in the
future (\Cref{s:future-work}).

The semantics of SMoL is as described in \Cref{s:intro}. SMoL includes
limited primitive operators (\Cref{t:smol-primitives}) to work with
numbers, strings, and vectors. It provides only one equality operator,
which tests for exact equality between atomic values and for pointer
equality between other values.

Students are given a working implementation of SMoL, built as a
\lstinline|#lang| language inside Racket
\citep{felleisenProgrammableProgrammingLanguage2018}. This language
provides only the defined syntax and semantics of SMoL, with no
(other) Racket features present. (As in Racket, arguments evaluate
left-to-right, to give stateful programs an unambiguous semantics.)
The implementation is available online:
\begin{center}
  \href{https://github.com/shriram/smol}{https://github.com/shriram/smol}
\end{center}

\section{Background: Misconceptions}%
\label{s:background-misconceptions}

\begin{table}[t]
  \centering
  \begin{tabularx}{\textwidth}{>{\hsize=.6\hsize\RaggedRight}X >{\hsize=.8\hsize\RaggedRight}X >{\hsize=.6\hsize\RaggedRight}X >{\hsize=1.3\hsize\RaggedRight}X}
    \textbf{Publication}                                                           &
    \textbf{Population}                                                            &
    \textbf{Languages}                                                             &
    \textbf{Topics}
    \\\hline
    \cite{fleuryParameterPassingRules1991}                                         &
    Likely CS2 students                                                   &
    Pascal                                                                         &
    Scope
    \\\hline
    \cite{goldmanIdentifyingImportantDifficult2008,goldmanSettingScopeConcept2010} &
    CS1 students                                                                   &
    Java, Python, Scheme                                                           &
    Scope and memory model (See their Figures 4 and 5)
    \\\hline
    \cite{fislerAssessingTeachingScope2017}                                        &
    Third- and fourth-year undergrads                                              &
    Java and Scheme                                                                &
    Scope, variable aliasing, and structure aliasing.
    \\\hline
    \cite{saarinenHarnessingWisdomClasses2019}                                     &
    CS2 students                                                                   &
    Java                                                                           &
    Variable aliasing and structure aliasing.
    \\\hline
    \cite{strombackProgressionStudentsAbility2023}                                 &
    CS masters                                                                     &
    Python                                                                         &
    Scope, variable aliasing, and structure aliasing.
    \\\hline
    \cite{strombackProgressionStudentsAbility2023}                                 &
    CS undergrads                                                                  &
    C++                                                                            &
    Scope, variable aliasing, and structure aliasing.
    \\\hline
  \end{tabularx}
  \caption{Topics of misconceptions found in prior research.}
  \label{t:rel-work}
\end{table}

Misconceptions related to scope, mutation, and higher-order functions have been
widely identified in varying populations
(from CS1 students to graduate students to users of online forums)
over many years (since at last 1991)
in varying programming languages (from Java to Racket) and in
different countries (such as the USA and Sweden).
\Cref{t:rel-work} lists works that seem most relevant to us.
Appendix A of \cite{sorvaVisualProgramSimulation2012} provides an
extensive survey of misconceptions reported
in research up to 2012.

% There are some small differences.
% \cite{fleuryParameterPassingRules1991} identifies a dynamic scope misconception
% that is different from \miscon{FlatEnv}:
% \begin{description}
%   \item[\surmisedMiscon{CallerEnv}] Function values don't remember their environments.
%     When a function is called, the function body is evaluated in an
%     environment that extends from the \textit{caller's} environment.
% \end{description}
% I don't include \surmisedMiscon{CallerEnv} in my analysis because in our
% data, all wrong answers that can be explained by \surmisedMiscon{CallerEnv}
% are also explainable by \miscon{FlatEnv}.

% There are overlaps between my survey and
% theirs. For instance, my \surmisedMiscon{CallerEnv} is their No.~47.
% However, because their descriptions are brief, it is difficult to tell
% whether a misconception in their survey matches my misconceptions.
% Because
% they provide neither misinterpreters nor representative program-output
% pairs, it is difficult to determine the overlaps precisely (showing
% the value of providing these two machine-runnable descriptions).
% At any rate,
% I certainly find
% no equivalent of \miscon{FlatEnv}, \miscon{DeepClosure}, and
% \miscon{DefByRef} in their survey.

Known misconceptions are often detected with a kind of instruments
called \emph{concept inventories}
\citep{hestenesForceConceptInventory1992,taylorComputerScienceConcept2014}.
In terms of mechanics, a concept inventory is just an instrument
consisting of multiple-choice questions (MCQs), where each question
has one correct answer and several wrong ones. However, the wrong ones
are chosen with great care. Each one has been validated so that if a
student picks it, I can quite unambiguously determine \emph{what
misconception the student has}. For instance, if the question is
``What is \lstinline|sqrt(4)|?'', then \lstinline|37| is probably an
uninteresting wrong answer, but if people appear to confuse
square-roots with squares, then \lstinline|16| would be present as an
answer.\footnote{Concept inventories are thus useful in many settings.
For instance, an educator can use them with clickers to get quick
feedback from a class. If several students pick a specific wrong
answer, the educator not only knows they are wrong, but also has a
strong inkling of \emph{precisely what} misconception that group has
and can address it directly.}

\section{Background: Tutoring Systems}%
\label{s:background-tutoring-systems}

There is an extensive body of literature on tutoring systems
(\cite{vanlehnBehaviorTutoringSystems2006a} is a quality survey), and
indeed whole conferences are dedicated to them. I draw on this
literature. In particular, it is common in the literature to talk
about a ``two-loop'' architecture
\citep{vanlehnBehaviorTutoringSystems2006a} where the outer loop
iterates through ``tasks'' (i.e., educational activities) and the
inner loop iterates through UI events within a task. I follow the same
structure in my Tutor (\Cref{s:tutor}).

Many tutoring systems focus on teaching programming (such as the
well-known and heavily studied LISP Tutor
\citep{andersonLISPTutor1985}), and in the process undoubtedly address
some program behavior misconceptions. My Tutor differs in a notable
way: it does not try to teach programming per se. Instead, it assumes
a basic programming background and focuses entirely on program
behavior misconceptions and correcting them. I am not aware of a
tutoring system (in computer science) that has this specific design.

\section{\textbf{RQ1}: Find Misconceptions}%
\label{s:plan-rq1}

\begin{description}
  \item[RQ1] \rqI
\end{description}

\subsection{In What Populations to Find?}%
\label{s:populations}

To find misconceptions that ``present (even) in students with prior
programming backgrounds'', I have been collecting data from multiple
populations that very likely have prior experience
(\Cref{s:generality-tutor}). I have the following data sources so far:
\begin{description}
  \item[University 1] students in a ``Principles of Programming
    Languages'' class at Brown University. The class has about 70--75
    students per year. It is not required, so students take it by
    choice. Virtually all are computer science majors. Most are in
    their third or fourth year of tertiary education; about 10\% are
    graduate students. All have had at least one semester of
    imperative programming, and most have significantly more
    experience with it. Most have had close to a semester of
    functional programming. The student work described here was
    required, but students were graded on effort, not correctness.
  \item[University 2] a primarily public university in the US. It is
    one of the largest Hispanic-serving institutions in the country. As
    such, its demographic is extremely different from those whose data
    were used above. The Tutor was used in one course in Spring 2023,
    taken by 12 students. The course is a third-year, programming
    language course. The students are required to have taken two
    introductory programming courses (C++ focused).
  \item[Textbook] an instrument was published on the website of a
    programming languages textbook. Over the course of 8 months,
    several hundreds of people submitted to the instrument. To protect
    privacy, I intentionally do not record demographic information,
    but I conjecture that the population is largely self-learners (who
    are known to use the accompanying book), including some
    professional programmers.
\end{description}

I am contacting professors from various other universities. Several of
them have expressed interest in using my instruments. So, the final
results will likely include more populations.

\subsection{Find Misconceptions by Analyzing Student-written Programs}%
\label{s:find-misconceptions-with-quizius}

In \Cref{s:background-misconceptions}, I discuss several papers that
have provided reports of student misconceptions with different
fragments of SMoL. However, it is difficult to know how comprehensive
these are. While some are unclear on the origin of their programs,
they generally seem to be expert-generated.

The problem with expert-generated lists is that they can be quite
incomplete. Education researchers have documented the phenomenon of
the \emph{expert blind spot} \citep{nathanExpertBlindSpot2001}:
experts simply do not conceive of many learner difficulties.

To overcome such bias, I analyze a University~1 dataset produced by a
two-year process to find (a) student-written programs that tend to
trip up students and (b) incorrect responses to those programs using a tool for the purpose
(\Cref{s:quizius}). I have been cleaning up those programs
(\Cref{s:collating-problems}) and confirming that
these programs are still tricky in the same way.
\Cref{s:misconceptions-tutor} presents the final list of
misconceptions found with this process.

\subsection{Find Misconceptions by Testing Students with Expert-written Programs}%
\label{s:find-misconceptions-with-tutor}

I have another source of misconceptions. I have been creating a Tutor
to answer my RQ2, which I explain in detail in \Cref{s:plan-rq2}. The
Tutor includes many tasks that ask students to predict the result(s)
of running a program. Although these programs originate from
student-written programs (\Cref{s:find-misconceptions-with-quizius}), they
have been modified to fit tutoring purposes. Perhaps because of the
modification or because of the population variance, I have found
several unanticipated patterns of errors
(\Cref{s:misconceptions-tutor}), which suggest unknown misconceptions.

I plan to change the Tutor so that it collects data to uncover these
unknown misconceptions and confirm known ones.

\subsection{Expected Results}%
\label{s:rq1-expected-results}

Eventually, I expect to build a table of misconceptions. Each
misconception comes with multiple programs. For each of these
programs, the wrong result that corresponds to the misconception should
be commonly predicted by students from various populations.
Furthermore, these students should give similar explanations for all
the wrong answers. For example, I might find a misconception called
\miscon{DefByRef} (See
\Cref{t:tutor-grounded-misconceptions-1,t:tutor-grounded-misconceptions-2,t:tutor-surmised-misconceptions}
in \Cref{s:misconceptions-tutor} for a list of misconceptions that I
have found): when students are asked to predict the result of
\begin{lstlisting}
  (defvar x 12)
  (defvar y x)
  (set! y 0)
  x
  y
\end{lstlisting}
X\% of them responded \lstinline|12 0|; when asked to predict
\begin{lstlisting}
  (defvar x 12)
  (defvar y x)
  (set! x 0)
  x
  y
\end{lstlisting}
Y\% of them responded \lstinline|0 12|; and these students all
explained something like ``\lstinline|y| is \lstinline|x|''.

To be reasonably specific about misconceptions, I have created a
definitional interpreter for each misconception that I have found. I
plan to do the same things for any new misconceptions. I call these
interpreters \emph{misinterpreters}. Having definitional interpreters
makes it convenient (and less error-prone) to associate wrong answers
with misconceptions (\Cref{s:misconceptions-tutor}).

\section{\textbf{RQ2}: Correct Misconceptions}%
\label{s:plan-rq2}

\begin{description}
  \item[RQ2] \rqII
\end{description}

To answer this RQ, I plan to create a Tutor (\Cref{s:design-tutor}) and
argue that the Tutor is effective (\Cref{s:evaluation}).

\subsection{Design a Tutor for Correcting Misconceptions}%
\label{s:design-tutor}

To answer this RQ, I have done some literature review on what
pedagogic techniques are helpful (\Cref{s:pedagogic-techniques}).
My current takeaway is that refutation text, case comparison, notional
machines, and language levels are helpful. I have created a Tutor that
incorporates all these ideas (\Cref{s:tutor}) and have collected data
from all populations.

I would like to first explain key aspects of the Tutor
(\Cref{s:tutorials,s:interpreting-tasks}) and then my plan to
incorporate ideas of refutation text (\Cref{s:refutationt-text}), case
comparison (\Cref{s:case-comparison}), and notional machines
(\Cref{s:notional-machine}). Finally, I present my plan to evaluate
the effectiveness of the Tutor and how evaluation impacts the Tutor
design (\Cref{s:evaluation}).

\subsubsection{Tutorials and Language Levels}%
\label{s:tutorials}

There are many misconceptions to correct. To make the tutoring process
manageable, I have designed the Tutor as a sequence of tutorials.
Inspired by prior research on language levels
(\Cref{s:pedagogic-techniques}), my sequence of tutorials is ordered,
and each tutorial covers one or more new language constructs
(\Cref{t:tutorials}). Each tutorial might be delivered as multiple modules to avoid forcing
students to continuously work for more than 30 minutes.

\begin{table}[t]
  \centering\begin{tabularx}{\linewidth}{p{0.4\linewidth} X}
    \textbf{Tutorial}                             & \textbf{Language Constructs}
    \\\hline
    \newterm{scope}: Lexical scope                &
    Primitives, variables, \lstinline|defvar|, and \lstinline|deffun|.
    \\\hline
    \newterm{mut-vars}: Mutable variables         &
    Adding \lstinline|set!|.
    \\\hline
    \newterm{vectors}: Vectors and vector updates &
    Adding vector operators.
    \\\hline
    \newterm{lambda}: Lambda expressions          &
    Adding \lstinline|lambda|.
    \\\hline
    \newterm{local}: Local binding forms          &
    Adding \texttt{let}.
    % \\\hline
  \end{tabularx}
  \caption{Tutorials and Their Language Constructs}
  \label{t:tutorials}
\end{table}

\subsubsection{Interpreting Tasks}%
\label{s:interpreting-tasks}

The Tutor mostly consists of \emph{interpreting tasks}:
multiple-choice questions (MCQs) that asks students to interpret
program. The options include (among other options
that I will explain shortly) the correct answer and wrong answers that
correspond to known misconceptions. Because students might conceive
wrong answers that I do not anticipate, the Tutor has been providing
an ``Other'' option that, once chosen, allows students to enter
arbitrary answers.

Let's reconsider this program from \Cref{s:rq1-expected-results}
\begin{lstlisting}
  (defvar x 12)
  (defvar y x)
  (set! y 0)
  x
  y
\end{lstlisting}
In this case, the options should include the correct answer
\lstinline|12 0| and a wrong answer \lstinline|0 0|, which corresponds
to the \miscon{DefByRef} misconception. If an MCQ only has few options
like this example, students have a great chance (in this case, 50\%)
of just guessing the correct answer or successfully using a process of
elimination. So I have been adding additional options that are not
obviously wrong. The Tutor is adding many options, but with an ad hoc
rule. I plan to revise the rules of adding extra options.

\paragraph{Penalty to Wrong Choices}\label{s:penalty} To discourage
students from making a random choice, the Tutor has been giving
penalties. Designing the strategy of assigning penalties is non-trivial:
\begin{itemize}
  \item A common strategy is to grade students by their correctness.
  However, grading by correctness is not suitable for a tutoring
  system because I do not mind students doing badly on earlier tasks
  as long as they eventually master the concepts. Actually, if
  students generally succeed in all tasks, the tutoring system is
  failing -- it does not teach anything to the students.
  \item An alternative common strategy is to grade by completion.
  However, grading by completion alone encourages students to rush
  through the tutorials and, hence, might encourage students to make
  random choices.
\end{itemize}
In short, I want to make students feel as follows:
\begin{itemize}
  \item They won't lose grades as long as they finish the tutorials.
  \item They should avoid giving wrong answers.
\end{itemize}
To encourage this feeling, I have been doing the following: when I
deployed the Tutor in my institution, University 1, I graded by
completion\footnote{I encourage collaborators to grade by
completeness, but I have no real control over how they actually use
the Tutor.}; when a student fails a task, the Tutor gives an
\emph{extra} task:
\begin{enumerate}

  \item The program is semantically the same as the first, but with
        superficial changes (e.g., variable names, constants, and
        operators are changed) so that the student cannot immediately
        guess the answer.

  \item Instead of multiple-choice, students must \emph{type} the
        answer into a text box. (The Tutor normalizes text to accommodate
        variations.) This is intentional. First, I want to force
        reflection on the explanation just given, whereas with an MCQ,
        students could just guess. Second, I feel that students would
        find typing more onerous than clicking. In case students had just
        guessed on a question, my hope is that the penalty of having to
        type means, on \emph{subsequent} tasks, they would be more likely
        to pause and reflect before choosing.

\end{enumerate}

\subsubsection{Design with Pedagogic Techniques Known to Work}

\paragraph{Refutation Text and Other Explanations}%
\label{s:refutationt-text}
The Tutor has been presenting a refutation text when a student chooses
a wrong answer that corresponds to known misconception(s). Let's
reconsider the program in \Cref{s:interpreting-tasks}. If a student
answers \lstinline|0 0|, which suggests they hold the
\miscon{DefByRef} misconception, the Tutor responds
\begin{quote}
  \lstinline|y| was bound to \lstinline|12| (i.e., the value of
  \lstinline|x|) rather than to \lstinline|x|. So changing the value
  of \lstinline|y| does not change the value of \lstinline|x|.
\end{quote}
If a student gives a wrong answer that does not correspond to a
misconception (either by choosing one of the generated options or by
choosing the ``Other'' option), the Tutor presents a generic
explanation. For example, if a student answers \lstinline|12 12|, the
Tutor responds
\begin{quote}
  The first definition binds \lstinline|x| to \lstinline|12|. The
  second definition binds \lstinline|y| to the value of \lstinline|x|,
  which is \lstinline|12|. The \lstinline|set!| mutates the binding of
  \lstinline|y|, so \lstinline|y| is now bound to \lstinline|0|.
\end{quote}
If a student gives the correct answer, the Tutor also presents the
generic explanation.

\paragraph{Case Comparison}%
\label{s:case-comparison}

The Tutor provides opportunities for making case comparisons: each
interpreting task presents a SMoL program and the program's result.
These program-result pairs are concrete cases of the semantics of
SMoL. To incorporate the idea of case comparison, the Tutor currently
prompts students to compare these cases and then to summarize the
rules of program behavior at the end of \emph{some} tutorials.

\paragraph{Notional Machine}%
\label{s:notional-machine}

Prior research argues (\Cref{s:pedagogic-techniques})that illustrating
how programs run with notional machines (NMs) is beneficial for
students. I have been giving students opportunities and sometimes
explicitly requiring them to run programs in the NM visualization tool.

\subsection{Evaluate the Tutor}%
\label{s:evaluation}

To fully answer my research questions, I should also argue that the
Tutor is effective at correcting misconceptions. In this section, I
describe how I want to refine the design of the Tutor to help me make
the argument, what data I want to collect, and what (statistical)
analysis I want to perform on the data.

I can infer whether students hold a misconception by analyzing their
responses to interpreting tasks. There are three possible responses to
an interpreting task:
\begin{enumerate}
  \item They give the correct answer.
  \item They give a wrong answer that corresponds to some
        misconceptions.
  \item They give a wrong answer that does not correspond to any (known)
        misconceptions.
\end{enumerate}
If a student gives a wrong answer that corresponds to exactly one
misconception \emph{and} an explanation that matches the
misconception, I deduce that the student holds the misconception when
doing the task.

Given this observation, I plan to set up the Tutor such that
\begin{itemize}
  \item Every wrong answer corresponds to up to one misconception. In
        this case, I say the wrong answer \emph{represents} the
        misconception.
  \item Every misconception is represented by some questions.
  \item In each tutorial, a misconception is either not represented by
        any interpreting tasks or represented by at least two tasks.
  \item Within each tutorial, the order of interpreting tasks is
        randomized.
\end{itemize}
For each tutorial and each misconception, I plan to do a model
comparison (with the standard AIC and BIC criteria) on the following
models/theories:
\begin{description}
  \item[TaskSpecific] Some students are more likely to choose the
  representing wrong answers in certain tasks.
  \item[PositionSpecific] Students are more likely to choose the
  representing wrong answer in task that appears earlier (or later) in
  the tutorial.
\end{description}

If the \textbf{TaskSpecific} model best explains the data, I deduce
that some programs are more confusing and that I need further
investigation into the relationship between the tasks and
misconceptions. If the \textbf{PositionSpecific} model best explains
\emph{and} students perform better in later tasks, I deduce that the
Tutor seems to be correcting misconceptions. If students perform
worse in later tasks, the Tutor is potentially doing harm.

To know students well, I would like to have as many tasks per
misconception as possible. But in practice, the number of tasks is
subject to the following limitations:
\begin{enumerate}
  \item I don't want the length of any tutorial section to exceed 30
  minutes.
  \item Programs shouldn't be too long. Some misconceptions are really
  similar and as a consequence have few short programs to tell them
  apart.
\end{enumerate}

The current Tutor is not prepared for this analysis yet. I describe how
I plan to change the Tutor in \Cref{s:future-work}.

\section{Preliminary Results}

\subsection{Generating Problems Using Quizius}
\label{s:quizius}

As said in \Cref{s:plan-rq1}, my main solution to the expert blind spot is to use the Quizius system
\citep{saarinenHarnessingWisdomClasses2019}. In contrast to the very
heavyweight process (involving a lot of expert time) that is generally
used to create a concept inventory, Quizius uses a lightweight,
interactive approach to obtain fairly comparable data, which an expert
can then shape into a quality instrument.

In Quizius, experts create a prompt; in my case, students were to
create small but ``interesting'' programs using the SMoL
language. Quizius shows this prompt to students and gathers their
answers. Each student is then shown a set of programs created by other
students and asked to predict (without running it) the value produced
by the program.\footnote{In the course (\Cref{s:populations}),
  students were given credit for using Quizius but not penalized for wrong
  answers, reducing their incentive to ``cheat'' by running
  programs. They were also told that doing so would diminish the value of
  their answers. Some students seemed to do so anyway, but most
  honored the directive.} Students are also asked to provide a rationale for why
they think it will produce that output.

Quizius runs interactively during an assignment period.
At each point, it needs to determine which
previously authored program to show a student.
It can
either ``exploit'' a given program that already has responses
or ``explore'' a new
one. Quizius thus treats this as a multi-armed bandit problem
\citep{katehakisMultiArmedBanditProblem1987} and uses that to choose a program.

The output from Quizius is (a) a collection of programs; (b) for
each program, a collection of predicted answers; and (c) for each
answer, a rationale. Clustering the answers is easy (after
ignoring some small syntactic differences). Thus, for
each cluster, I obtain a set of rationales.

After running Quizius in the course (\Cref{s:populations}),
I took over as experts. Determining which is the right
answer is easy, whereas expert knowledge is useful is in
\emph{clustering the rationales}. If all the rationales for a wrong
answer are fairly similar, this is strong evidence that there is a
common misconception that generates it. If, however, there are
multiple rationale clusters, that means the program
is not discriminative enough to distinguish the misconceptions, and it
needs to be further refined to tell them apart. Interestingly, even
the correct answer needs to be analyzed, because sometimes correct
answers do have incorrect rationales (again, suggesting the program
needs refinement to discriminate correct conceptions
from misconceptions).

Prior work using Quizius \citep{saarinenHarnessingWisdomClasses2019}
finds that students do author programs that the experts did not
imagine. In my case, I seeded Quizius with programs from prior papers
(\Cref{s:background-misconceptions}), which gives the first few
students programs to respond to. However, I found that Quizius
significantly expanded the scope of my problems and misconceptions.
In my final instrument, most programs were directly or indirectly
inspired by the output of Quizius.

The Quizius instrument was set up by other people. I analyzed the
collected data.

\subsection{Collating Problems}
\label{s:collating-problems}

While Quizius is very useful in principle, it also produced data that
needed significant curation for the following reasons:
\begin{itemize}

  \item A problem may have produced diverse outputs simply because it
        was written in a very confusing way. Such programs do not reveal
        any useful \emph{behavior} misconceptions, and must therefore be
        filtered out. For instance:

        \begin{lstlisting}
  (defvar x 1)
  (defvar y 2)
  (defvar z 3)
  (deffun (sum a ...) (+ a ...))
  (sum x y z)
  \end{lstlisting}

        A reader might think that \lstinline|sum| takes variable arguments (so
        the program produces \lstinline|6|), but in fact \lstinline|...| is a single
        variable, so this produces an arity error.

  \item Some programs relied on (or stumbled upon) underspecified aspects
        of unimportant (in particular, non-standard) parts of SMoL, such as floating-point versus
        rational arithmetic.

  \item A problem may have produced diverse outputs simply because it is
        hard to parse or to (mentally) trace its execution. One example was a
        17-line program with 6 similar-looking and -named functions. As
        another example:

        \begin{lstlisting}
  (defvar a (or (/ 1 (- 0.25 0.25)) (/ 1 0.0)))
  (defvar b (and (/ 1 (- 0.25 0.25)) (/ 1 0.0)))
  (defvar c (and (/ 1 0.0) (/ 1 (- 0.25 -0.25))))
  (defvar d (or (/ 1 0) (/ 1 (- 0.25 -0.25))))
  (and (or a c) (or b d))
\end{lstlisting}

        This program is not only confusing, it \emph{also} tests interpretations of (a) exact
        versus inexact numbers and (b) truthy/falsiness, leading to
        significant (but not very useful) answer diversity.

  \item As noted above, a program's wrong (or even correct) answers may
        correspond to multiple (mis)conceptions. In these cases, the program
        must be refined to be more discriminative.

  \item I felt the existing programs were insufficient to cover the
        ideas I wanted students to work through. For instance, I found no program that aliases vectors using \lstinline|defvar|, so we
        added this program:

\begin{lstlisting}
  (defvar x (mvec 100))
  (defvar y x)
  (vec-set! x 0 200)
  y
\end{lstlisting}

  \item I renamed vector operators to avoid confusion between
        \lstinline|vset!|, which replaces a vector element, and \lstinline|set!|,
        which mutates variables. I rename all vector operators from, for example,
        \lstinline|vset!| to \lstinline|vec-set!|.

  \item I deferred the local binding forms to the end, since I felt
        they were less essential. I therefore rewrote programs that use
        it to not depend on it.

        % \item I removed some questions that I deemed ``Lispy'' (e.g.,
        %   involving the difference between parenthetical and infix syntax).

  \item I removed some programs that relied on underspecified aspects or
        unimportant aspects of SMoL. For example, some depended on
        whether operations on pairs could be applied to arbitrary vectors:

\begin{lstlisting}
  (pair? (mpair 1 2))
  (pair? (mvec 1 2))
  (pair? '#(1 2))
  (pair? '(1 2))
\end{lstlisting}

        As another example, one hinged on
        whether or not a function's formal parameter could have the same
        name as the function itself:

\begin{lstlisting}
  (deffun (f f) f)
  (f 5)
\end{lstlisting}

  \item I resolved ambiguities in some programs by adding
        answer choices or even other questions to tease
        out different interpretations.

  \item To reduce the number of concepts, I removed programs that
        relied upon \emph{im}mutable vectors and lists, because they
        did not seem to create problems. (For brevity, I leave these
        out of the presentation of SMoL in \Cref{s:smol}, though they
        are in the implementation.)

  \item I removed questions related to function equality, which was
        not a focus of the Tutor.

  \item I removed programs of a ``Lispy'' nature.

\end{itemize}

\subsection{Preliminary List of Misconceptions}%
\label{s:misconceptions-tutor}

I now examine what I learned from the Tutor's interpreting tasks in
terms of program behavior misconceptions.

I iteratively created our final catalog of misconceptions (from the
perspective of the data in this document). I started with misinterpreters
representing the misconceptions for which we
have reasonable validation (due to the prose in Quizius), so I call
them \emph{grounded} misconceptions. I then looked at wrong answers
not covered by these but chosen by students, and did my best to
distill these into misconceptions. These are \emph{surmised}
misconceptions (which we identify with a $^\ddag$), which need to be
validated in the future. I then re-ran the misinterpreters against the
chosen answers. I terminated when all the remaining wrong answers were
either (a) found in very few students (we found a gap between 23\% and
13\%, and hence took 14\% as the threshold), (b) difficult for us to
attribute to a misconception, or (c) appeared to me to be ``Lispy''
and hence not of broad interest.

\Cref{t:tutor-grounded-misconceptions-1,t:tutor-grounded-misconceptions-2,t:tutor-surmised-misconceptions}
list the final catalog. For each, I also present a Tutor question for
which the marked wrong answer can be explained by \emph{only} the
named misconception. That is, that program-answer pair is a
representative example of that misconception.

\begin{table}[!ht]
  \centering\begin{tabularx}{\linewidth}{X l X}
    \textbf{Misconception}                                                     &
    \textbf{Question}                                                          &
    \textbf{Table of Answers}
    \\\hline
    \miscon{CallByRef} Function calls alias variables.                         &
    % mut-vars.not_aliased_by_funarg_2 (35)
    \lstinputlisting{programs/call-by-ref.txt}                                 & {
        \begin{tabularx}{\linewidth}{r X}
          78\%   & \lstinline|12|    \\
          11\%   & \lstinline|error| \\
          **10\% & \lstinline|0|     \\
          1\%    & \lstinline|23|    \\
        \end{tabularx}
      }
    \\\hline
    \miscon{CallsCopyStructs} Function calls copy data structures.             &
    % vectors2.alias_with_funcall (48)
    \lstinputlisting{programs/calls-copy-structs.txt}
                                                                               & {
        \begin{tabularx}{\linewidth}{r X}
          90\%   & \lstinline|#(173 0)| \\
          **10\% & \lstinline|#(1 0)|   \\
        \end{tabularx}
    }                                                                              \\\hline
    \miscon{DeepClosure} Closures copy the \textit{values} of free variables.  &
    % lambda3.lambda_remembers_env (70)
    \lstinputlisting{programs/deep-closure.txt}                                & {
        \begin{tabularx}{\linewidth}{r X}
          86\%   & \lstinline|4|      \\
          **10\% & \lstinline|3|      \\
          3\%    & \lstinline|error|  \\
          1\%    & \lstinline|lambda| \\
        \end{tabularx}
    }                                                                              \\\hline
    \miscon{DefByRef} Variable definitions alias variables.                    &
    % mut-vars.not_aliased_by_defvar_1 (29)
    \lstinputlisting{programs/def-by-ref.txt}                                  & {
        \begin{tabularx}{\linewidth}{r X}
          85\%   & \lstinline|0 12|                \\
          **12\% & \lstinline|0 0|                 \\
          2\%    & \lstinline|error|               \\
          1\%    & depends on implementation. \\
        \end{tabularx}
      }
    \\\hline
    \miscon{DefOrSet}  Both definitions and variable assignments are
    interpreted as follows: if a variable is not defined in the
    current environment, it is defined. Otherwise, it is mutated to
    the new value.
                                                                               &
    % program: mut-vars.update_undefined (28)
    \lstinputlisting{programs/def-or-set.txt}                                  & {
        \begin{tabularx}{\linewidth}{r X}
          85\%   & \lstinline|error| \\
          **15\% & \lstinline|2|     \\
        \end{tabularx}
      }
    \\\hline
    \miscon{DefsCopyStructs} Variable definitions copy structures recursively. &
    % vectors2.alias_with_defvar (47)
    \lstinputlisting{programs/defs-copy-structs.txt}                           & {
        \begin{tabularx}{\linewidth}{r X}
          **67\% & \lstinline|#(100)| \\
          30\%   & \lstinline|#(200)| \\
          1\%    & \lstinline|#(300)| \\
          1\%    & \lstinline|error|  \\
        \end{tabularx}
    }                                                                              \\\hline
  \end{tabularx}
  \caption{Ground misconceptions identified by the SMoL~Tutor. Answers marked
    with ``**'' represent the misconception. (Part I)}
  \label{t:tutor-grounded-misconceptions-1}
\end{table}

\begin{table}[!ht]
  \centering
  \begin{tabularx}{\linewidth}{X l X}
    \textbf{Misconception}                                                                 &
    \textbf{Question}                                                                      &
    \textbf{Table of Answers}
    \\\hline
    \miscon{FlatEnv} There is only one environment, the global
    environment. (This misconception is a kind of dynamic scope.)                          &
    % scope.error_when_refer_to_undefined (9)
    \lstinputlisting{programs/flat-env.txt}                                                & {
        \begin{tabularx}{\linewidth}{r X}
          96\%  & \lstinline|error| \\
          **4\% & \lstinline|402|   \\
        \end{tabularx}
      }
    \\\hline
    \miscon{FunNotVal} Functions are \textit{not} considered
    first-class values. They can't be bound to other variables, passed as arguments, or referred to by data structures.
                                                                                           &
    % lambda1.fun_as_parameter (62)
    \lstinputlisting{programs/fun-not-val.txt}                                             & {
        \begin{tabularx}{\linewidth}{r X}
          83\%   & \lstinline|4|     \\
          **11\% & \lstinline|error| \\
          4\%    & \lstinline|2|     \\
          1\%    & \lstinline|8|     \\
        \end{tabularx}
    }                                                                                          \\\hline
    \miscon{IsolatedFun} Functions can't refer to free variables
    except for the built-in ones.
                                                                                           &
    % scope.refer_global_is_possible (5)
    \lstinputlisting{programs/isolated-fun.txt}                                            & {
        \begin{tabularx}{\linewidth}{r X}
          77\%   & \lstinline|3|     \\
          **23\% & \lstinline|error| \\
        \end{tabularx}
      }
    \\\hline
    \miscon{NoCircularity} Data structures can't (possibly indirectly)
    refer to themselves.                                                                   &
    % vector2.warmup_circularity (53)
    \lstinputlisting{programs/no-circularity.txt}                                          & {
        \begin{tabularx}{\linewidth}{r X}
          76\%   & \lstinline|3|                   \\
          **14\% & \lstinline|error|               \\
          9\%    & Run out of memory or time. \\
          1\%    & \lstinline|+inf|                \\
        \end{tabularx}
    }                                                                                          \\\hline
    \miscon{StructByRef} Data structures might refer to variables by their references.     &
    % vector2.alias_var_in_mvec (49)
    \lstinputlisting{programs/struct-by-ref.txt}                                           & {
        \begin{tabularx}{\linewidth}{r X}
          67\%   & \lstinline|#(1 2 3)| \\
          **24\% & \lstinline|#(1 2 4)| \\
          9\%    & error           \\
        \end{tabularx}
    }                                                                                          \\\hline
    \miscon{StructsCopyStructs} Storing data structures into data structures makes copies. &
    % lambda1.smol_quiz_circularity (58)
    \lstinputlisting{programs/structs-copy-struct.txt}                                     & {
        \begin{tabularx}{\linewidth}{r X}
          65\%  & \lstinline|#0=#(#0# #0#)| (Racket circular object notation) \\
          24\%  & \lstinline|#(#(2 3) #(2 3))|                                \\
          **6\% & \lstinline|#(#(2 #(2 3)) #(2 3))|                           \\
          6\%   & error                                                  \\
        \end{tabularx}
    }                                                                                          \\\hline
  \end{tabularx}
  \caption{Ground misconceptions identified by the SMoL~Tutor. Answers marked
    with ``**'' represent the misconception. (Part II)}
  \label{t:tutor-grounded-misconceptions-2}
\end{table}

\begin{table}[!ht]
  \centering
  \begin{tabularx}{\linewidth}{X l X}
    \textbf{Misconception}                                                             &
    \textbf{Question}                                                                  &
    \textbf{Table of Answers}                                                              \\\hline
    \surmisedMiscon{NestedDef} Sequences of definitions are
    interpreted as if they are written in nested blocks. A definition
    is not in the scope of later definitions.
                                                                                       &
    % program: scope.what_is_x_4 (13)
    \lstinputlisting{programs/nested-def.txt}                                          & {
        \begin{tabularx}{\linewidth}{r X}
          92\%  & \lstinline|2| \\
          **8\% & \lstinline|1| \\
        \end{tabularx}
    }                                                                                      \\\hline
    \surmisedMiscon{Lazy} Expressions are only evaluated when their values are needed. &
    % order.bad_order (20)
    \lstinputlisting{programs/lazy.txt}                                                & {
        \begin{tabularx}{\linewidth}{r X}
          **57\% & \lstinline|1 3|   \\
          43\%   & \lstinline|error| \\
        \end{tabularx}
    }                                                                                      \\\hline
  \end{tabularx}
  \caption{Surmised misconceptions identified by the SMoL~Tutor. Answers marked
    with ``**'' represent the misconception.}
  \label{t:tutor-surmised-misconceptions}
\end{table}

\paragraph{A New Potential Misconception}

For the following program, added in the Tutor:
\begin{lstlisting}
  (defvar y (+ x 2))
  (defvar x 1)
  x
  y
\end{lstlisting}
56\% of students asserted that it produces
\texttt{1 3}. Based on this, I surmise that students might have
another misconception, which I define as \surmisedMiscon{Lazy}. (Recall that
SMoL is eager, but even in many lazy languages, this would be an error.)

\paragraph{Another Potential Misconception, and Its Effect on
  Interpreting Descriptions.}%
\label{ss:misinterpreters:submisconception}

Consider this program from the SMoL Tutor:
\begin{lstlisting}
(defvar x 1)
(deffun (main)
  (deffun (get-x) x)
  (defvar x 2)
  (get-x))

(main)
\end{lstlisting}
This program, suitably translated, would produce \lstinline|2| in a wide
variety of languages (Python, JavaScript, Racket, Java, etc.), because
\lstinline|get-x| and the inner \lstinline|x| are in the same scope block. The
answer \lstinline|1| cannot be explained by any of my existing
misconceptions. Based on this, I surmise a new misconception,
\surmisedMiscon{NestedDef}. (Though its frequency falls below our
threshold, my reading of answers suggests this may be more
widespread, and I feel it needs to be investigated more.)

Once I turned this into a misinterpreter, I found that it
unexpectedly captured the program-answer pair for the following
program --- which should be an error, due to the
double-binding of \lstinline|x| in the same scope block---and the answer
\lstinline|2 0|:
\begin{lstlisting}
  (defvar x 0)
  (defvar y x)
  (defvar x 2)
  x
  y
\end{lstlisting}
Previously, I had interpreted this only as \miscon{DefOrSet}, because
students had stated that the second \lstinline|(defvar x ....)|
``mutates'' or ``redefines'' \lstinline|x|. This is reminiscent of the
behavior of languages like Python, which use the same syntax both for
binding new variables and for mutating existing ones.

The problem here is that the word ``redefine'' underspecifies how the
second definition is interpreted. I had interpreted it as
\textit{mutating} the binding established by the first definition,
which fits \miscon{DefOrSet}. However, another possibility is that
it \emph{shadows} the binding (i.e., establishes a new scope block). We
did not recognize that the original misconception is underspecified
until I uncovered the new (surmised) misconception.

\subsection{What Pedagogic Techniques are Effective?}%
\label{s:pedagogic-techniques}

The fundamental problem about correcting misconceptions is: how do you
fix a misconception? One approach is to only ``present the right
answer'', for fear that discussing the wrong conception might actually
reinforce it. Instead, there is a body of literature starting from
\cite{posnerTheoryConceptualChange1982} that presents a theory of
conceptual change, at whose heart is the \emph{refutation text}. A
refutation text tackles the misconception directly, discussing and
providing a refutation for the incorrect idea. Several studies
\citep{schroederRefutationTextFacilitates2022} have shown their
effectiveness in multiple other domains.

\emph{Case comparisons} draw analogies between examples.
\cite{alfieriLearningCaseComparisons2013a} suggest that asking
(rather than not asking) students to find similarities between cases,
and providing principles \emph{after} the comparisons (rather than
before or not at all), are associated with better learning outcomes.

Several groups of authors have argued for the benefits of visualizing
program executions
\citep{dicksonExperiencesImplementingUtilizing2022,
guoOnlinePythonTutor2013, karnalimUsePythonTutor2017,
napsExploringRoleVisualization2002}. Visualization seems to
increase engagement and help students understand how programs work.
These work often mention \emph{notional machines}
\citep{duboulayBlackBoxGlass1981}, which refers to the underlying
mental models rather than the visual representations.

Prior research argues for the benefits of \emph{language levels}
\citep{findlerDrSchemeProgrammingEnvironment2002,crestaniExperienceReportGrowing2010}:
a language has better be taught as a sequence of languages, where each
later language includes more language constructs than the previous
one.

\subsection{The SMoL Tutor}%
\label{s:tutor}

This section describe the current status of the Tutor.

The Tutor includes five tutorials, shown on the left in
\Cref{t:tutor-goal-sentences}. The larger topics are further broken
down into 2--3 modules. The goal was for students to spend at most
20--30 minutes per module. my data show that in practice, students
spent about 9.8 (median) minutes.

\Cref{f:smol-tutor-example} illustrates an interpreting task as
described in \Cref{s:interpreting-tasks}. The Tutor also asked students
to perform some other activities, like showing programs and asking for
their heap content, which I do not cover in this document.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{images/smol-tutor-example.png}
  \caption{Screenshots of an interpreting question in SMoL Tutor. The
    top-left shows the initial state, where the question is presented
    as an MCQ. If a student chooses a wrong answer, they will receive
    feedback (bottom-left) and will be asked a similar question
    (right). The similar question must be answered by typing.}
  \label{f:smol-tutor-example}
\end{figure}

In addition to asking students questions, the Tutor along the way
introduces terminology and states the true conceptions. These are the
teaching goals of the Tutor. I therefore refer to these as
\newterm{goal sentences}. \Cref{t:tutor-goal-sentences} lists
the (abbreviated) goal sentences for each tutorial. I plan to keep
goal sentences in the Tutor.

\begin{table}[t]
  \centering\begin{tabularx}{\linewidth}{p{0.2\linewidth} X}
    \textbf{Tutorial}                                              & \textbf{Goal Sentences}
    \\\hline
    \newterm{scope}: Variable definitions and function definitions &
    \begin{enumerate}[topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
      \item Referring to an unbound variable leads to an error.
      \item Define "blocks".
      \item SMoL is lexically scoped rather than dynamically scoped.
      \item SMoL disallows defining a variable twice in one block.
      \item SMoL is eager (and evaluates from left to right) rather
            than lazy, reactive, or relational.
    \end{enumerate}
    \\\hline
    \newterm{mut-vars}: Variable updates                           &
    \begin{enumerate}[topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
      \item Variable assignment respects the hierarchical structure of blocks.
      \item Variables do not alias.
    \end{enumerate}
    \\\hline
    \newterm{begin}: Sequencing expressions                        &
    A sequencing expression evaluates its sub-expressions from left to right and
    returns the value of the last sub-expression.
    \\\hline
    \newterm{vectors}: Vectors and vector updates                  &
    \begin{enumerate}[topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
      \item Define heap and memory addresses.
      \item Vectors can be referred to by vectors, including themselves.
      \item Vectors are not copied but aliased by bindings.
      \item Constructing vectors doesn't alter environments.
      \item Introducing bindings doesn't alter the (value part of) heap.
    \end{enumerate}
    \\\hline
    \newterm{lambda}: Lambda expressions                           &
    \begin{enumerate}[topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
      \item Functions are (first-class) values.
      \item Functions remember the environment where they were created.
      \item Functions can be created with lambda expressions.
      \item A function definition can be viewed as a variable definition plus a lambda expression.
    \end{enumerate}
    \\\hline
    \newterm{local}: Local binding forms                           &
    Introduce \texttt{let}, \texttt{letrec}, and \texttt{let*}.
    % \\\hline
  \end{tabularx}
  \caption{SMoL Tutor tutorials and their goal sentences.}
  \label{t:tutor-goal-sentences}
\end{table}

Some later tutorials include questions about earlier tutorials so
that I can check whether students remember the concepts across
modules. In particular, the \textbf{mut-vars} tutorial starts with questions
about \textbf{scope}, and \textbf{lambda} starts with questions about
\textbf{mut-vars} and \textbf{vectors}. I plan to keep this design to
check retention.

\subsection{How Effective Is the Tutor?}
\label{s:effect-tutor}

\begin{figure}[p]

  \includegraphics[width=\linewidth]{images/Misconceptions_Decay.png}

  \caption{How many students chose a wrong answer that (uniquely)
    represents a misconception? (Downward tendency suggests improvement
    over time.)}
  \label{f:tutor-trend-plots}

\end{figure}

% Recall that the SMoL Tutor is not only a collection of MCQs: it is
% also a \emph{Tutor}! So far, I have investigated the value of the
% MCQs. Now I examine its tutoring aspect. Concretely, I ask:
% \begin{center}

%   How effective is the Tutor at correcting each misconception?

% \end{center}
% To study this, I perform the following analysis per
% misconception. Using misinterpreters, I identify those questions
% whose wrong answers fit only one misconception (i.e., only one output
% matches that produced by that misinterpreter, and it matches only that
% misinterpreter). I then examine student performance over time across
% those questions. This would let me examine how they do on just that
% topic, in isolation, over time.

This section presents part of my preliminary answer to \textbf{RQ2}
(\Cref{s:plan-rq2}): is the Tutor effective?

I present the result in two forms. Graphically, we show plots in
\Cref{f:tutor-trend-plots}. Each figure shows the percentage of
students who chose the answer corresponding to that misconception.
Ideally, we would like to see these percentages diminish.

Indeed, that is what I see in most of the graphs. The exceptions are
\miscon{CallsCopyStructs}, \surmisedMiscon{NestedDef}, and
\miscon{StructsCopyStructs}, which have only one problem (and hence no
trend), and \miscon{DefOrSet} and \miscon{FunNotVal}, which show an
increase. The lack of improvement for \miscon{FunNotVal} is
unsurprising because the Tutor does not explicitly address this issue,
focusing on closures created by \lstinline|lambda|s rather than named
functions. (However, this does not explain the increase!)

I also perform a logistic regression to see whether these
improvements are significant (at a $p < 0.05$ threshold). Of the 11:
\begin{itemize}

  \item Of the nine seemingly improved (i.e., decreased)
        misconceptions:
        \begin{itemize}
          \item Two are \emph{not} significant: \miscon{CallByRef}
                (\pValue\ = .074); \miscon{NoCircularity} (\pValue\ = .075).
          \item The other seven \emph{are} significant.
        \end{itemize}
  \item However, the two with an increasing trend (\miscon{DefOrSet}
        and \miscon{FunNotVal}) are \emph{also} significant.
\end{itemize}

Overall, the data suggest that the Tutor \emph{most likely did not do
harm} and perhaps even \emph{may have done some good}. Revisions on
the Tutor (\Cref{s:future-work}), in light of misinterpreters, is
necessarily to give a more accurate evaluation. For example I found
far fewer problems than I would have liked. Concretely, only 40 of the
71 eligible problems (after removing the non-SMoL modules) were useful
in the above analysis.

% These data broadly suggest that the Tutor is a net positive.
% Ultimately, however, what these data really show is a need for
% improvement in the Tutor. When designing the Tutor questions, many
% more were intended to be representative of single misconceptions.
% However, as noted in \Cref{s:misinterp-intro}, it is easy to be
% incomplete (or incorrect) in ascribing misconceptions. Furthermore, as
% their set grows, it is difficult to reassess all the problems
% manually. Once evaluated using misinterpreters,

% I therefore view the above data as
% purely formative: they . However, it would be improper to read too much into the
% analysis. It is quite possible that some of the other problems would
% have found issues. Rather, what I really see is value in the
% misinterpreter concept. It is not only useful for analysis, it is also
% valuable for \emph{problem design}: in the next iteration of the
% Tutor, I will use misinterpreters actively to shape the incorrect answers and,
% as necessary, update the programs as well. I therefore hope to have
% much more thorough analyzes in future work.

\subsection{How Generalizable are My Results?}
\label{s:generality-tutor}

% There is, of course, a significant danger that the data above have
% been overfitted to University 1, thereby actually reflecting the state
% of its curriculum rather than some greater truth about program
% behavior understanding.

% I already have some reason to believe this is
% not the case: the related work discussed in
% \Cref{s:background-misconceptions} is drawn from many institutions in
% multiple countries with different educational preparations, levels,
% and demographics. Nevertheless, that gives me only limited information
% about the specific questions and misconceptions described above.

% Fortunately, I was able to deploy the Tutor on two other
% populations: University 2 and Texbook (See \Cref{s:populations} for their details).
% \begin{itemize}

%   \item University 2 is a primarily public university in the US. It is
%         one of the largest Hispanic-serving institutions in the country. As
%         such, its demographic is extremely different from those whose data
%         were used above. The Tutor was used in one course in Spring 2023,
%         taken by 12 students.
%         The course is a third-year, programming
%         language course. The students are required to have taken two
%         introductory programming courses (C++ focused).

%   \item A separate instance of the Tutor was published on the
%         website of a programming languages textbook [ANON]. Over the course
%         of 8 months, 597 people started with the first module and 103
%         users made it to the last one. To protect privacy, I intentionally
%         do not record demographic information, but I conjecture that the
%         population is largely self-learners  (who are known to use the
%         accompanying book),
%         including some professional
%         programmers. It is extremely
%         unlikely to be the students from either university, because they
%         would not get credit for their work on the public instance; they
%         needed to use the university-specific instance. Furthermore, since
%         they were not penalized for wrong answers, it would make little
%         sense to do a ``test run'' on the public instance. Finally, I note
%         that there is no overlap between the dates of submission on the
%         public instance and the semester at University 1.

% \end{itemize}
% These two populations are therefore at least somewhat different from
% the original population, and help me assess whether the problems we
% identify are merely an artifact of the first institution.

% To evaluate,
I computed a Spearman's rank correlation $\rho$, ranking questions by
what percentage of students got the question right. Between the
original university and University 2, we obtain a \pValue~=~2.013e-07.
Between the original university and the online population, I obtain a
\pValue~<~2.2e-16. These show that the other two populations performed
similarly to the original one. While further validation on other
populations remains essential, these suggest that the questions are
\emph{finding misconceptions that may be universal}.

Of course, other threats to generalizability remains: both University
1 and University 2 use the same textbook; SMoL's Lispy syntax might
have bias in probing errors. I plan to address these issues in the
future (\Cref{s:future-work}).

% \section{Threats to Validity}
% \label{s:threats}

% \subsection{Construct Validity}

% Did I measure the right thing?
% While my goal is to study student understanding of language
% behavior, what I actually measure is performance on MCQs on select
% programs. MCQs as a mechanism introduce various clear biases, though
% I add the ``Other'' options to somewhat alleviate them. The small
% size, syntactic details (such as intentionally meaningless variable
% names), and other aspects of the programs may also impact my ability
% to measure understanding. (This could go both ways: students may have
% no trouble understanding behavior in a small program but may struggle
% to do so in a larger one.)

% A particularly notable threat is the use of Lispy
% syntax. I chose it for two reasons: both because it helps clarify
% scope (\Cref{s:smol}) and because the rest of the course used
% Racket. However, students may well perform differently with more
% familiar syntaxes. It seems unlikely their performance would be \emph{too}
% different given the many languages that have produced similar
% misconceptions (\Cref{t:rel-work}). Nevertheless, I intend to use a
% variety of syntaxes to examine this issue further.

% Finally, I curated programs by hand, so they may well reflect my own
% biases about misconceptions. It may be possible to mitigate this
% problem by synthesizing MCQ programs using the misinterpreters.

% \subsection{Internal Validity}

% Is my reasoning valid? I have applied standard techniques and
% measurements for evaluating student responses. However, our
% instruments still lack the validity of a proper concept
% inventory. Their creation requires heavyweight processes (such as
% Delphi methods \citep{goldmanSettingScopeConcept2010}) that require many hours of expert attention as
% well as conversations with learners, and can hence be prohibitive in
% cost. The Quizius method (\Cref{s:quizius}) was created precisely to
% be an inexpensive method that provides a good proxy.

% Ultimately, my goal is to provide a reasonable instrument for
% widespread use. While the set of all misconceptions could be
% unbounded, I believe my tasks, especially as embedded in the Tutor,
% provide a good starting point for others. In particular, if students
% select a wrong answer, that is still of \emph{some} use to an
% educator, even if the precise misconception cannot be pinned down with
% the highest accuracy. I therefore believe my instruments, and our
% misinterpreter technique, are of general value.

% A failure in my Tutor's logging infrastructure led me to miss some
% responses. These are unlikely to be task-specific because the Tutor
% has a generic framework that should perform the same across
% tasks. Moreover, on average only $0.4\%\ (\textrm{sd} = 0.6\%)$ of values are missing. Therefore, I do not believe
% this had a noticeable impact. (In addition, every wrong answer is
% still wrong! I may just not have exactly the right proportions of
% them.)

% Because I don't randomize Tutor question order, it is hard to tell
% whether the general decreasing trend in \cref{f:tutor-trend-plots}
% shows misconceptions being corrected, or shows properties of the
% questions (e.g., maybe later questions in the plots are easier because
% their programs happen to be shorter).

% \subsection{External Validity}

% How well do my results generalize? Certainly there is reason to
% question whether my results would apply to other
% populations. \Cref{s:generality-tutor} provides preliminary evidence
% that the results are not specific to one institution, and
% \Cref{s:background-misconceptions} suggests these issues are widespread. Nevertheless,
% much more broad testing is needed to confirm my specific instruments.

% The other major concern is the tie to Lispy syntax. Learners in other
% settings may do worse or even better with it. Building a Tutor that
% supports multiple, and more traditional, syntaxes should help address
% this issue. I defer this to future work (\cref{s:future-work}).

\section{Related Work}%
\label{s:related-work}

\subsection{Program Behavior}

Our idea of ``goal sentences'' is not substantially different from the
\textit{rules of program behavior} from \cite{duranRulesProgramBehavior2021}.
I only used these sentences (or rules) to guide the
design of the Tutor and as text in the Tutor itself.
\cite{duranRulesProgramBehavior2021} argue for other uses of such
sentences.

\subsection{Misinterpreters}

Our idea of misinterpreters is related to mystery languages
\citep{diwanPLdetectiveSystemTeaching2004,
  pombrioTeachingProgrammingLanguages2017}. Both approaches use
evaluators that represent alternative semantics to the same syntax.
However, the two are complementary.
In mystery languages, instructors design the space of semantics with
pedagogic intent, and
students must create programs to explore that space. Misinterpreters,
in contrast, are driven by student input, while the programs are provided
by instructors. The two approaches also have different goals:
mystery languages focus on encouraging students to experiment with
languages; misinterpreters aim at capturing students' misconceptions.

\section{Future Work \& Timeline}%
\label{s:future-work}

\subsection{Keep Exploring More Pedagogic Techniques}

I have learned (\Cref{s:pedagogic-techniques}) that refutation text,
case comparison, notional machines, and language levels are helpful. I
plan to explore more literature and improve the Tutor if I find new
helpful techniques.

\subsection{Implemented But Not Evaluated Improvements in the Tutor}

\paragraph{Ease Access to the Notional Machine}

The preliminary Tutor provides a ``Copy This Program'' button that
helps students to run the program in DrRacket, where a SMoL
implementation is provided.

This approach is inconvenient both for the students and for me. For
students, they have to click, paste, and run, which is less convenient
than a single click.

Given these shortcomings, I have created a web-based notional machine
\begin{center}
  \href{https://lukuangchen.github.io/stacker-2023}{https://lukuangchen.github.io/stacker-2023}
\end{center}
The notional machine can be configured with URL parameters. For
example,
\href{https://lukuangchen.github.io/stacker-2023/?program=\%28defvar+x+12\%29\%0A\%28defvar+y+x\%29\%0A\%28set\%21+y+0\%29\%0Ax\%0Ay}{this
link} opens the NM with a program embedded.

The latest Tutor provides a button on the top-right of each program.
If students click the button, they will open a new browser tab that
shows the NM populated with the program. I have deployed the latest
Tutor to the latest population at Brown (i.e., University 1), but I
have not analyzed the data, so the new Tutor is not presented in
the preliminary results.

\paragraph{Reduce Syntactic Bias with Multiple Syntax}

The processes described in
\Cref{s:find-misconceptions-with-quizius,s:find-misconceptions-with-tutor}
have been conducted with SMoL's Lispy syntax (\Cref{s:smol}). This can
potentially introduce biases to the found misconceptions. For example,
I found that some students think the following program
produces \lstinline|'(4 5)| rather than \lstinline|'(1 2)|.
\begin{lstlisting}
  (filter (lambda (n) (> 3 n)) '(1 2 3 4 5))
\end{lstlisting}
This checks whether $\verb|3| > \verb|n|$, but those students
presumably vocalized it as ``greater than \lstinline|3|,
\lstinline|n|?''. Although I have tried to remove programs like this
from the Tutor, there is still a risk of having biases in student
errors. This semester (Fall 2023) I am already deploying a newer SMoL
Tutor at Brown. This new Tutor is able to display programs in
JavaScript and Python, in addition to the Lispy syntax. I plan to
compare in Spring 2024 the new Tutor data with the current results,
i.e., a comparison between uni-syntax Tutor and multi-syntax Tutor.

\subsection{The Rest of Fall 2023: Improve the Tutor}

\paragraph{Log Access to the Notional Machine}

I have not changed the logging systems to log when students click the
buttons to access the NM. I plan to make the change before Spring 2024.

\paragraph{Limit Access to the Notional Machine}

Another issue with the current design is that students might run a
program before giving an answer. I don't know how often this
happens. But, given that students receive a penalty (\Cref{s:penalty})
for giving wrong answers, they might do so when the program is overly
complicated. This problem is more of an issue after the switch to a
web-based notional machine because now the ``cost'' of running a
program is cheaper. My plan is to change the Tutor so that it provides
a program-specific link \emph{only} after each interpreting task.

\paragraph{Use Case Comparisons More Consistently}

The Tutor currently prompts students to compare program-output pairs
and then to summarize the rules of program behavior at the end of
\emph{some} tutorials.

I plan to expand this to \emph{all} tutorials and use a consistent
prompt. The prompt should ask about the behavior of new language
constructs and their interaction with all previously introduced
constructs.

The current scope tutorial uses the following prompt:
\begin{quote}
  When I see a variable reference, how do I find the corresponding
  declaration, if any?
\end{quote}
This prompt has a few problems:
\begin{itemize}
  \item It is unclear to adapt it to other tutorials.
  \item Students tend to give brief responses possibly because the
  questions are asking for a lot and because students are tired when
  they reach the end of a tutorial.
\end{itemize}

So I plan to change the question format into a notice-and-wonder style
of questions, which is known to be less demanding
(\Cref{s:pedagogic-techniques}). So I plan to replace the
\textbf{scope} prompt with
\begin{quote}
  \emph{Think about} the following questions:
  \begin{itemize}
    \item How do \lstinline|defvar|s work?
    \item How do \lstinline|deffun|s work?
    \item How do variable referencess (e.g., \lstinline|x| in \lstinline|(+ x 1)|) work?
    \item How do \lstinline|defvar|s, \lstinline|deffun|s, and variable references interact with each other?
  \end{itemize}
  If you believe you know the answers perfectly, please \emph{response with}
  \begin{quote}
    I know the answers with 100\% certainty.
  \end{quote}
  Otherwise, please \emph{respond with} what you are uncertain about these language constructs.
\end{quote}
I plan to use a similar prompt in later tutorials. For example, in \textbf{mut-vars},
I would want the Tutor to ask
\begin{quote}
  \emph{Think about} the following questions:
  \begin{itemize}
    \item How do \lstinline|set!|s work?
    \item How do \lstinline|set!|s interact with other language constructs (namely, \lstinline|defvar|s, \lstinline|deffun|s, and variable references)?
  \end{itemize}
  If you believe you know the answers perfectly, please \emph{respond with}
  \begin{quote}
    I know the answers with 100\% certainty.
  \end{quote}
  Otherwise, please \emph{respond with} what you are uncertain about these language constructs.
\end{quote}

\paragraph{Use More Sensible Options in Interpreting Tasks}

The Tutor is adding extra options to interpreting tasks in an ad hoc
way. Eventually, I plan to generate options by moving and/or copying
numbers in the current options. In the case of the example question in
\Cref{s:interpreting-tasks}, my plan would add \lstinline|12 12| and
\lstinline|0 12|. In summary, each MCQ has the following options:
\begin{itemize}
  \item the correct answer,
  \item wrong answers that correspond to known misconceptions,
  \item options generated by moving and/or copying numbers in the
        current options
  \item the ``Other'' option (which, once chosen, allows students to
        enter arbitrary answers)
\end{itemize}

\paragraph{Ask For Explanation on Choices}

The Tutor currently does not ask students to explain when they
        give a wrong answer in an interpreting question. This limits my
        ability to find new misconceptions and to confirm that the
        tasks do detect the anticipated misconceptions. I plan to
        change the Tutor so that it asks for an explanation in each
        interpreting task.

\paragraph{Clean-Up the Program Set}
        % With student explanations, I check whether
        % choosing the wrong answer indicates holding the corresponding
        % misconception(s). I plan to eyeball the explanations to find
        % new misconceptions. This piece of data also helps me confirm
        % that anticipated wrong answers represent the expected
        % misconceptions.
As I said in \Cref{s:effect-tutor}, for many of the
        misconceptions, the Tutor has too few questions. On the other
        hand, there are many questions that do not produce useful data
        because their wrong answers do not uniquely point to a
        misconception. I plan to adjust the collection of interpreting
        questions so that every misconception has at least two
        questions that aim to correct the misconception.

\paragraph{Randomize the Order of Interpreting Tasks}

The Tutor is not randomizing the order of questions. This makes
        it difficult to compare questions for one misconception --
        earlier questions might be difficult simply because the
        programs are somehow more difficult. I plan to randomize the
        order of interpreting tasks within each tutorial module.
After randomizing question orders, I would need to check
        carefully the dependencies between explanations and
        resolve these dependencies, if any.

\paragraph{Clean-Up Variable Names}

The Tutor uses many variable names; some are separated by dashes
        (e.g., \lstinline|get-x|). Students might misread these
        variables. For example, some students might misread
        \lstinline|(get-x)| (a function named \lstinline|get-x| is
        called with no argument) as \lstinline|(get x)| (a function
        named \lstinline|get| is called with one argument
        \lstinline|x|). These misreadings add noise to the data. So I
        plan to use consistent and simple variable names in all
        programs.

\subsection{Spring 2024: Collect More Data}

Several faculties from other institutions have expressed interest in
using the Tutor in Spring 2024. My goal is to collect data from at
least two other institutions.

I am officially taking a personal leave to do military service in my
country in Spring 2024, so I do not plan to make more progress
other than data collection.

\subsection{Summer 2024: Data Analysis and Write a Paper}

During this time, I plan to analyze the data I collected in Fall 2023
and Spring 2024 and start working on a paper about the changes in the Tutor.

\subsection{Fall 2024: Collect More Data and Write a Paper}

During Fall 2024, I plan to collect a new round of data from Brown, analyze
data that I collected in Spring 2024, and hopefully finish writing the new paper.

\subsection{Spring 2025: Defense}

I plan to defend in Spring 2025.

\bibliography{bibfile}

\end{document}
